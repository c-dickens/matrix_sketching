%!TEX root = sketching-convex-ols.tex
\newcommand{\eat}[1]{}
\newcommand{\SRHT}{SRHT\xspace}

Our work is founded on the use of random projections to obtain
subspace embedding (Section~\ref{sec: preliminaries}).
In this section, we perform empirical studies to answer three central
questions:
(1) time cost: does the sketching approach provide speed-ups in
practice;
(2) accuracy: how large does the projection dimenions need to be in
order to achieve small error;
(3) subspace embedding:
according to the theoretical analysis, the CountSketch requires a
higher projected dimension than other sketch methods to obtain a
subspace embedding -- but is this borne out in practice? 

\eat{
As outlined in Section \ref{sec: preliminaries} the key quantity that we would
like to understand is the subspace embedding.
The central questions we seek to answer are: (1) whether the time complexity
in practise matches the theory, (2) how large should the projection
dimension be in order to obtain small error (3) the CountSketch
theoretically requires a larger projection to obtain the subspace embedding
property, is this reflected in practise?}

\medskip
\noindent\textbf{Time cost for sparse data.}
We begin by evaluating the scalability of sketching for inputs that
are large and sparse.
We immediately remove the (dense) Gaussian sketch from consideration,
since the time cost for explicit matrix multiplication is excessively
high even for moderately sized inputs.
We subsequently focus our attention on CountSketch and \SRHT. 
To construct a subspace embedding, 
the CountSketch takes
time $O(\nnz{A})$, while the \SRHT requires $O(nd \log n)$.
That is, CountSketch explicitly scales (linearly) with the sparsity of
the data, whereas \SRHT, while fast, does not directly depend on the
sparsity.
We expect CountSketch to be preferable for very sparse data,
and investigate whether there is a point where the input density is
such that \SRHT is faster. 

\noindent
\textit{Experimental setup.}
We generated random $n \times d$ matrices with $n=50,000$ and varying $d$
in $\{ 10, 100, 1000, 5000 \}$.
That is, the aspect ratio varies from $2 \times 10^{-4}$ to $1\times
10^{-1}$.
This is consistent with the range observed for the real data sets studied (see
Table \ref{table: data-facts}). 
We varied the \textit{density} of the synthetic data.
We define density $\rho$ simply as the fraction of entries in the matrix $A$
that are non-zero.
We used the \texttt{scipy.sparse.random} routine to instantiate $A$,
where non-zero entries are chosen iid normal. 
Note that the speed of our sketching methods does not depend on the data
distribution, so we are not overly concerned with this choice for
these timing experiments. 
We initially choose the projection dimension $m = 5d$; this parameter
is investigated in more detail in subsequent sections. 

\noindent
\textit{Timing results.}
Figure~\ref{fig: summary-time-50000} shows the timing results as
density varies for the four different input dimensions $d$.
The results appear consistent with the theory.
There is an increasing
trend for the time to build the CountSketch as the density of the data
increases.
Meanwhile, the time for the \SRHT embedding remains stable across
density values. 
%
However, our main take-away from this experiment is that CountSketch
is \textit{an order of magnitude faster}, even in the densest regime. 
In particular, we do not observe any cross over point where the \SRHT
embedding is faster to build than CountSketch for a given input
dimensionality. 
We saw the same property for varying values of $n$, so we do not
report them. 
Similar behavior is also seen on real datasets (Table
\ref{table: real-data-subspace-embedding}), where CountSketch is
uniformly faster than \SRHT, sometimes by multiple orders of
magnitude.
This is most pronounced for the ``tall-and-skinny'' (i.e. low aspect
ratio) data sets. 
In absolute terms, the time cost to build the CountSketch is below one
second in all but the largest data sets. 
Note that in a few experiments with large values of $d$ ($d>1000$),
the \SRHT failed to complete, and so results for these tests are
omitted. 
The limiting factor appears to be the computation of Hadamard
transforms: these are individually relatively fast, but in aggregate create a
pinch point. 


\begin{figure}
  \centering
\includegraphics[scale=0.75,keepaspectratio]{summary_time_density_50000}
        \caption{$n=50000$ and $d$ from $10,100,1000,5000$.  The legend
        describes the sketch used and the number appended is the value of $d$
        chosen for that particular experiment.
        A projection dimension of $m=5d$ was used.
        The mean time over 5 trials has been reported.}
        \label{fig: summary-time-50000}
\end{figure}





% \begin{figure}
%   \includegraphics[scale=0.25,keepaspectratio]{summary_time_density_10000}
%   \caption{Time to compute summaries for $n=100000$, $d$ given in legend next
%   to sketch name and sketch dimension $m = 5d$ used.} \label{fig: summary-times-vs-sparsity}
% \end{figure}




% \begin{figure}
%     \centering
%     \begin{subfigure}[b]{0.49\textwidth}
%         \includegraphics[scale=0.45,keepaspectratio]{summary_time_density_10000}
%         \caption{$n=100000$}
%         \label{fig: summary-time-100000}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.49\textwidth}
%         \includegraphics[scale=0.425,keepaspectratio]{distortion_vs_cols}
%         \caption{Distortion compared to number of columns}
%         \label{fig: distortion-columns}
%     \end{subfigure}
%     ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
%       %(or a blank line to force the subfigure onto a new line)
% \end{figure}

% Please add the following required packages to your document preamble:
%


\begin{table}[ht]
\centering
\begin{adjustbox}{width=1\textwidth}
\small

\begin{tabular}{|c|c|c|c|c|c|c||c|c|c|c|c|c|}
  \hline
\multirow{2}{*}{Dataset} & \multicolumn{3}{c|}{CountSketch (time)} & \multicolumn{3}{c||}{SRHT (time)} & \multicolumn{3}{c|}{CountSketch (error)} & \multicolumn{3}{c|}{SRHT (error)} \\
                        &     $1d$       &  $2d$            & $5d$          & $1d$          & $2d$        & $5d$         &     $1d$       &  $2d$          &   $5d$         &  $1d$          & $2d$         & $5d$  \\
\hline

Complex & 0.028 & 0.046 & 0.111 & 0.264 & 0.235 & 0.309 & $0.924_5$ & $0.464_5$ & $0.186_5$ & $0.829_5$ & $ 0.367_5$ & $0.0877_5$ \\

Landmark & 0.522 & 1.22 & 3.68 & - & - & - & $0.382_5$ & $0.190_5$ & $0.076_5$ & - & - & - \\

Slice &  0.067  &  0.073  &  0.117 &   1.34   & 1.29 & 1.27 &  $0.004_5$  &  $0.003_5$ &  $0.0006_5$ &   $0.004_5$ & $0.005_5$  &  $0.0005_5$  \\

Rail2586  & 0.897 & 1.31 & 3.15 & - & - & - & \textbf{0.085} & 0.043 & 0.017 & - & - & - \\

California Housing     &   0.001          &  0.001           &   0.002          & 0.018         &  0.022        & 0.017          &   \textbf{0.135}          &  0.024           & 0.008       & \textbf{0.242}          & 0.172         & 0.049 \\


YearPredictionsMSD      &    0.143         &   0.143          &  0.144          &  3.06         &  3.02        &   2.99        &  \textbf{0.031}           &   0.049          &    0.007           &   \textbf{0.033}        &   0.027        &  0.003         \\

US Census   & 0.261 & 0.257 & 0.240 & 7.08 & 7.23 & 6.62 & \textbf{0.032} &  0.152 & 0.037 & \textbf{0.063} & 0.081 & 0.041 \\

Susy  & 0.444 & 0.440 & 0.423 & 10.7 & 10.9 & 10.9 & \textbf{0.200} & 0.103 & 0.028 & \textbf{0.196} & 0.056 & 0.041 \\

\hline

w1a & 0.002 & 0.003 & 0.005 & 0.031 & 0.027 & 0.031 & $0.050_5$ & $0.026$ & $0.010_1$ & 0.047 & $0.026_2$ & $0.009_3$ \\

w2a & 0.001 & 0.002 & 0.004 & 0.032 & 0.033 & 0.037 & \textbf{0.052} & 0.030 & 0.009 & \textbf{0.047} & 0.024 & 0.007 \\

w3a & 0.001 & 0.002 & 0.006 & 0.066 & 0.058 & 0.073 & $0.048_2$ & $0.026_2$ & \textbf{0.001} & 0.057 & $0.024_2$ & \textbf{0.01} \\

w4a & 0.001 & 0.003 & 0.005 & 0.064 & 0.061 & 0.072 & 0.048 & $0.030_1$ & \textbf{0.011} & \textbf{0.049} & 0.024 & 0.001 \\

w5a & 0.002 & 0.003 & 0.005 & 0.126 & 0.133 & 0.138 & \textbf{0.047} & 0.024 & 0.009 & \textbf{0.051} & 0.024 & 0.010 \\

w6a & 0.002 & 0.004 & 0.006 & 0.329 & 0.298 & 0.331 & \textbf{0.053} & 0.022 & 0.010 & \textbf{0.050} &  0.026 & 0.012 \\

w7a & 0.003 & 0.004 & 0.006 & 0.369 & 0.366 & 0.342 & \textbf{0.050} & 0.024 & 0.011 & \textbf{0.049} & 0.023 & 0.010 \\

w8a & 0.005 & 0.006 & 0.010 & 0.956 & 0.930 & 0.994 & \textbf{0.052} & 0.024 & 0.010 & \textbf{0.046} & 0.024 & 0.010 \\
\hline
\end{tabular}
\end{adjustbox}
\caption{Comparison of subspace embedding results using CountSketch and SRHT
on real datasets.
Bold text indicates
the lowest projection dimension $m$ for which rank is preserved at this level
\textit{as well as all projection dimensions larger than $m$} in every trial.
We indicate the number of failures by a subscript: \textit{no} bold text and
\textit{no} subscript indicates that for this $m$ a subspace embedding was
achieved, however, for at least one of the larger projections there was a trial
in which rank was lost (for instance $(\text{w1a, SRHT}, 1d)$).
Observe that the three fail cases are the datasets  with the largest three
aspect ratios.}
\label{table: real-data-subspace-embedding}
\end{table}

% -----------------------------------------------------------------------------

\medskip
\noindent\textbf{Error and Embedding Dimension}
Although the theory shows that obtaining a subspace embedding is
sufficient property to instantiate the IHS scheme,
the analysis does not reveal how big the \textit{sampling factor}
$\gamma$ must be relative to $d$ in order to ensure a subspace
embedding is obtained.
That is, we seek to empirically determine constants that are otherwise
hidden within the $O(\cdot)$ notation. 
We study when the $SA$ matrix achieves full rank, as we vary input
parameters such as the aspect ratio of the matrix $A$. 

\textit{Experimental Setup.}
We carry out this experiment on synthetic data where we can easily
vary aspect ratio, and then confirm our findings on the real
datasets.
For this set of experiments we fix $n$ and then generate a selection of matrices
of full density
ranging from `tall-and-skinny' to `fat' matrices by choosing $d$ to vary
from roughly $0.05n$ to $0.5n$ over different distributions.
The empirical value of $\eps$ can be measured: this is referred to as the
\textit{distortion} and is the error induced by the approximate matrix product
computation $\|A^T A - A^T S^T S A \|_F/\|A^TA\|_F$.
Results on the distortion for input matrices $A$ whose entries chosen iid normal are given
in Figure \ref{fig: distortion-sampling-factor}.
Each experiment is repeated 10 times, and the mean distortion reported.
We also measured the rank of the returned matrix $SA$, and discuss
when it is not preserved (i.e. $S$ does not provide a subspace
embedding). 

\textit{Accuracy and Dimensionality Results.}
The plots indicate that in general, the distortion induced by the CountSketch is
comparable with the Gaussian sketches however both are a constant factor worse
than the SRHT across each of the settings for this distribution.
In addition, the SRHT and Gaussian sketches retain rank in every instantiation
of the experiment yet this is not the case for the CountSketch which retains
rank in all but one (Figures \ref{fig: subspace-125e3-aspectratio} and \ref{fig: subspace-375e1-aspectratio} but at the lowest
sampling factor) of the trials when the aspect ratio is 0.125 or 0.25.
When the aspect ratio exceeds 0.25 we begin to see more rank deficiency for
lower sampling factors and as such require $\gamma > 1.10$ for a CountSketch
subspce embedding but only $\gamma = 1.01$ for SRHT/Gaussian subspace embeddings as
seen in Figure \ref{fig: subspace-375e1-aspectratio}.
This behaviour is exacerbated in the `fat' case, Figure \ref{fig: subspace-5e1-aspectratio},
when $d/n = 0.5$ under which the CountSketch \textit{never} returns a subspace
embedding but both of the other methods are stable in this regime.


The CountSketch achieves comparable distortion to the other methods and constructs
an embedding in vastly less time, however, there is a price to pay for this
computational benefit.
As the aspect ratio increases the number of rank deficient embeddings also
grows; taking $\gamma > 1.05$ is sufficient to ensure that rank is preserved
when $d/n=0.25$ yet the required sampling factor grows considerably with $d/n$.
When $d/n=0.375$ we need roughly $\gamma = 1.125$ for all of the embeddings to
preserve rank for the CountSketch but when $d/n = 0.5$ \textit{none} of the
tested sampling factors up to $\gamma=1.25$ returned a subspace embedding.
This failure of the CountSketch to accurately preserve rank at higher aspect
ratios highlights one deficiency it possesses compared to the other sketches.
Put simply, \textit{CountSketch is more sensitive to higher aspect ratios and
requires a higher sampling factor in this regime compared to the other
sketching methods.}
However, given that the sketches are designed for particularly tall and skinny
matrices, one could argue that CountSketch failing above $d/n = 0.375$ is in
fact not problematic: on all the test instances which fall into the tall-and-
skinny setup, CountSketch retains rank with comparable distortion.

On real datasets we see similar behaviour in that the CountSketch performs well
when the aspect ratio is particularly small but the performance of rank
retention deteriorates as the aspect ratio increases.
For all but the highest aspect ratio datasets there exists a $\gamma$ such that for any
other $\gamma' > \gamma$ tested, a subspace embedding was returned for $\gamma$
and $\gamma'$.
However, on such high aspect ratio data (Slice ($d/n = 7 \times 10^{-3}$),
 Complex ($d/n=1.2 \times 10^{-1}$),and w1a ($d/n = 1.2 \times 10^{-1}$)) neither
 method performs particularly well with respect to returning a subspace embedding.
We further remark that on tall-and-skinny datasets CountSketch seems to return an
embedding at the same sampling factor as the SRHT with distortion that differs
by at most a small factor of $10^{-2}$ in the worst case (California Housing,
w3a), but more often on the order of $10^{-3}$.
One can see the progression of the rank retention through analysing the w$n$a
 datasets; neither method returns a full rank embedding for w$1$a at or above
 a particular sampling factor.
 Although both methods return a full rank embedding for w$3$a at $\gamma=5$,
 the w$4$a data requires $\gamma = 5$ for the CountSketch and $\gamma = 1$
 for the SRHT: this is the largest disparity between the two methods and occurs
 at $d/n = 4.1 \times 10^{-2}$.
 Despite the linear dependence on $\delta$ failure probability for a CountSketch
 subspace embedding compared to the $\log(1/\delta)$ of the other methods,
 the largest aspect ratio for which the CountSketch behaves almost identically
 to the SRHT is $d/n = 3 \times 10^{-2}$: less than this and we see only small
 differences in the distortion of the embedding.
 As a result, given the added benefit of a huge time saving, in the regime when
 $d/n < 3 \times 10^{-2}$, the CountSketch seems favourable for a subspace
 embedding becasuse it is fast and rank is retained at low sampling factors.
 If one were \textit{solely} interested in a subspace embedding but had
 slightly `fatter' data, along with less time constraints, then the SRHT is
 favourable.
 In spite of this, in the iterative framework a rank-deficient `embedding' might
 suffice providing that any lost directions in one iteration are picked up at
 later iterations.
 From this perspective, it appears that the CountSketch is favourable irrespective
 of the dimensionality because it is much faster and has comparable distortion
 to the SRHT even when rank is lost.

\medskip
\textbf{Summary.}
 We conclude the section by referring to the questions asked originally of the
 embedding behaviour.
 First we observed that the CountSketch embedding scaled with the density of the
 data but this growth was mild enough that for all aspect ratios tested it was
 a factor of 10 faster than the SRHT (which also scaled consistently with the
 theory).
 Secondly, for a variety of real datasets we have given a rough guide of how
 large the sampling factor $\gamma$ must be to ensure a subspace embedding is
 returned and shown examples when a `small' sampling factor is not sufficient.
 Finally, on the data we have tested, we have shown that there are examples
 when the CountSketch requires a larger embedding dimension to obtain a subspace
 embedding compared to the SRHT, however, this is only a small constant factor
 larger for the CountSketch despite the theory suggesting a factor $d$ larger.
 In addition, we have also shown empirically that there is a threshold below
 which the performance of the CountSketch is almost identical (in that rank is
 preserved and distortion is only a small constant factor different) to the SRHT
 but is vastly cheaper to compute.



\begin{figure}
        \centering
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{subspace_embedding_results/subspace_embedding_dimension_gaussian_2048_256.pdf}
            \caption{$d=256$}
            \label{fig: subspace-125e3-aspectratio}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{subspace_embedding_results/subspace_embedding_dimension_gaussian_2048_512.pdf}
            \caption{$d=512$}
            \label{fig: subspace-375e1-aspectratio}
        \end{subfigure}
        \vskip\baselineskip
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{subspace_embedding_results/subspace_embedding_dimension_gaussian_2048_768.pdf}
            \caption{$d=768$}
            \label{fig: subspace-25e1-aspectratio}
        \end{subfigure}
        \quad
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{subspace_embedding_results/subspace_embedding_dimension_gaussian_2048_1024.pdf}
            \caption{$d=1024$}
            \label{fig: subspace-5e1-aspectratio}
        \end{subfigure}
        \caption{Distortion compared to sampling factor for various aspect ratios.
        Plotted are the mean results over 10 trials and a cross denotes when at least
        one trial has failed.
        The larger crosses indicate a higher number of failures with the largest
        crosses in Figure \ref{fig: subspace-5e1-aspectratio} for which CountSketch
        \textit{never} returned a full rank subspace embedding.}
        \label{fig: distortion-sampling-factor}
    \end{figure}
