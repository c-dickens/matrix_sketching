%!TEX root = sketching-convex-ols.tex

As outlined in Section \ref{sec: preliminaries} the key quantity that we would
like to understand is the subspace embedding.
The central questions we seek to answer are: (1) whether the time complexity
in practise matches the theory, (2) how large should the projection
dimension be in order to obtain small error (3) the CountSketch
theoretically requires a larger projection to obtain the subspace embedding
property, is this reflected in practise?

\noindent\textbf{Time Complexity compared to sparsity.}
Only the CountSketch and SRHT are tested because the explicit matrix
multiplication for the Gaussian sketch is exceedingly high which even
for moderately sized inputs will be prohibitive compared to the other two methods.
The CountSketch takes
$O(\nnz{A})$ time to construct a subspace embedding
whereas the SRHT requires $O(nd \log n)$.
Although the SRHT does not scale with the sparsity of the data, for the
practitioner it
will be of interest to know where, if any, there is a threshold at which one of
the transforms is preferable from the perspective of time cost.
To test this claim we generated random matrices of size $n=50000$ with varying $d$
in $\{ 10, 100, 1000,5000 \}$.
This captures the range of aspect ratios seen in the real datasets that we will
test on as described in Table \ref{table: data-facts}.
The \textit{density} parameter $\rho$ (varied from 0 to 1) denotes the
fraction of nonzeros in the test matrix $A$ which was generated using the
\texttt{scipy.sparse.random(n,d,density=$\rho$)} routine which returns a
matrix of $nd\rho$ nonzero entries chosen independently from a standard normal
distribution.
Both sketching methods are agnostic to the distribution of choice so from a
speed perspective this makes no difference.
A projection dimension of $5d$ was chosen for the summarisation but sufficient
sketching dimensions will be discussed in the succeeding section; for now one can
assume that $m=5d$ returns a subspace embedding under this experimental setup.
As seen in Figure \ref{fig: summary-time-50000}, both transforms scale
consistently with the theory: the CountSketch has increased time to compute the
summary as the density of the data increases and the time taken to compute the
SRHT embedding is generally stable as the density varies.
One key point to take away is that the CountSketch is \textit{an order
of magnitude faster} to compute the embedding compared to the SRHT even as the
density increases;
as $n$ grows to be significantly larger than $d$, this saving could be vast.
Interestingly, our implementations do not show a cross over point for corresponding
settings of the parameters i.e for a fixed $(n,d,m)$ triple the time to compute the
CountSketch summary is always significantly less than that for
the SRHT summary.
This behaviour is consistent across each of the setup values so only those for
the $n = 50000$ are given here.

In addition, the time saving of using the CountSketch over the SRHT is reflected
on the real datasets with the results given in Table
\ref{table: real-data-subspace-embedding}.
CountSketch embeddings are found at a fraction of the time for the SRHT which
allows us to sketch the data in less than 1 second on all but two of the
experimental setups.
Moreover, the (at least factor 10) speedup is in general observed on each of the
real datasets which is consistent with the synthetic data.
For the `tall-skinny' matrices the CountSketch again found to be at least a
factor of 10 faster.
For the wider matrices ($d>1000$) the SRHT times out in our
implementation so we report no results.
Although the embedding time difference is quite severe in some cases, we note
that the time complexity of our SRHT implementation is comparable to the test
cases of the Hadamard transform that we use given in its repository.


\begin{figure}
  \centering
\includegraphics[scale=0.75,keepaspectratio]{summary_time_density_50000}
        \caption{$n=50000$ and $d$ from $10,100,1000,5000$.  The legend
        describes the sketch used and the number appended is the value of $d$
        chosen for that particular experiment.
        A projection dimension of $m=5d$ was used.}
        \label{fig: summary-time-50000}
\end{figure}





% \begin{figure}
%   \includegraphics[scale=0.25,keepaspectratio]{summary_time_density_10000}
%   \caption{Time to compute summaries for $n=100000$, $d$ given in legend next
%   to sketch name and sketch dimension $m = 5d$ used.} \label{fig: summary-times-vs-sparsity}
% \end{figure}




% \begin{figure}
%     \centering
%     \begin{subfigure}[b]{0.49\textwidth}
%         \includegraphics[scale=0.45,keepaspectratio]{summary_time_density_10000}
%         \caption{$n=100000$}
%         \label{fig: summary-time-100000}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.49\textwidth}
%         \includegraphics[scale=0.425,keepaspectratio]{distortion_vs_cols}
%         \caption{Distortion compared to number of columns}
%         \label{fig: distortion-columns}
%     \end{subfigure}
%     ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
%       %(or a blank line to force the subfigure onto a new line)
% \end{figure}

% Please add the following required packages to your document preamble:
%


\begin{table}[ht]
\centering
\begin{adjustbox}{width=1\textwidth}
\small

\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
  \hline
\multirow{2}{*}{Dataset} & \multicolumn{3}{c|}{CountSketch (time)} & \multicolumn{3}{c}{SRHT (time)} & \multicolumn{3}{c|}{CountSketch (error)} & \multicolumn{3}{c|}{SRHT (error)} \\
                        &     $1d$       &  $2d$            & $5d$          & $1d$          & $2d$        & $5d$         &     $1d$       &  $2d$          &   $5d$         &  $1d$          & $2d$         & $5d$  \\
\hline
YearPredictionsMSD      &    \textbf{0.143}         &   0.143          &  0.144          &  \textbf{3.06}         &  3.02        &   2.99        &  0.031           &   0.049          &    0.007           &   0.033        &   0.027        &  0.003         \\


California Housing     &   \textbf{0.001}          &  0.001           &   0.002          & \text{0.018}         &  0.022        & 0.017          &   0.135          &  0.024           & 0.008       & 0.242          & 0.172         & 0.049 \\

Slice &  0.067  &  0.073  &  0.117 &   1.34   & 1.29 & 1.27 &  0.004  &  0.003 &  0.0006 &  0.004 & 0.005  &  0.0005  \\

Susy  & \textbf{0.444} & 0.440 & 0.423 & \textbf{10.7} & 10.9 & 10.9 & 0.200 & 0.103 & 0.028 & 0.196 & 0.056 & 0.041 \\

Landmark & 0.522 & 1.22 & 3.68 & - & - & - & 0.382 & 0.190 & 0.076 & - & - & - \\

Complex & 0.028 & 0.046 & 0.111 & 0.264 & 0.235 & 0.309 & 0.924 & 0.464 & 0.186 & 0.829 & 0.367 & 0.0877 \\

US Census   & \textbf{0.261} & 0.257 & 0.240 & \textbf{7.08} & 7.23 & 6.62 & 0.0320 &  0.152 & 0.037 & 0.063 & 0.081 & 0.041 \\

Rail2586  & \textbf{0.897} & 1.31 & 3.15 & - & - & - & 0.085 & 0.043 & 0.017 & - & - & - \\

wx1 & & & & & & & & & & & & \\

wx2 & & & & & & & & & & & & \\

wx3 & & & & & & & & & & & & \\

wx4 & & & & & & & & & & & & \\

wx5 & & & & & & & & & & & & \\

wx6 & & & & & & & & & & & & \\

wx7 & & & & & & & & & & & & \\

wx8 & & & & & & & & & & & & \\  
\hline
\end{tabular}
\end{adjustbox}
\caption{Comparison of CountSketch and SRHT on real datasets. Bold text indicates
the fastest embedding which preserves rank.
Observe that the three fail cases are the datasets  with the largest three
aspect ratios.}
\label{table: real-data-subspace-embedding}
\end{table}

% -----------------------------------------------------------------------------


\noindent\textbf{Sufficient Sketching Dimension}
Although our theory shows that a subspace embedding is sufficient to use a random
projection within the IHS scheme, we first need to understand how large the
sampling factor $\gamma$ must be relative to $d$ in order to obtain a subspace
embedding.
The constants within the $O(\cdot)$ term must be understood to ensure that $SA$
is of full rank and we invesitgate how this changes depending on the aspect
ratio of the matrix.
For this set of experiments we fix $n$ and then generate a selection of matrices
which range from `tall-and-skinny' to `fat' matrices by choosing $d$ to range
from roughly $0.05n$ to $0.5n$ over a variety of distributions.
The results from choosing matrices whose entries are standard normal are given
in Figure \ref{fig: distortion-sampling-factor}.
The value of $\eps$ was then measured: this is referred to as the \textit{
distortion} and is the error induced by the approximate matrix product
computation $\|A^T A - A^T S^T S A \|_F/\|A^TA\|_F$.
In addition, we also measure the rank of the returned matrix $SA$ as it must
be preserved for a true subspace embedding.
The plots indicate that in general, the distortion induced by the CountSketch is
comparable with at least one of the Gaussian or SRHT sketches.
There are regimes in which, purely from a distortion perspective, one of the
sketches is consistently about a constant factor better (namely the SRHT on the
Gaussian random matrices) but it seems that the distortion is similar for each
of the sketches tested.

The CountSketch achieves comparable distortion to the other methods and constructs
an embedding in vastly less time, however, there is a price to pay for this
computational benefit.
As the aspect ratio increases the number of rank deficient embeddings also
grows; taking $\gamma > 1.05$ is sufficient to ensure that rank is preserved
when $d/n=0.25$ yet the required sampling factor grows considerably with $d/n$.
When $d/n=0.375$ we need roughly $\gamma = 1.125$ for all of the embeddings to
preserve rank for the CountSketch but when $d/n = 0.5$ \textit{none} of the
tested sampling factors up to $\gamma=1.25$ returned a subspace embedding.
This failure of the CountSketch to accurately preserve rank at higher aspect
ratios highlights one deficiency it possesses compared to the other sketches.
Put simply, \textit{CountSketch is more sensitive to higher aspect ratios and
requires a higher sampling factor in this regime compared to the other
sketching methods.}
However, given that the sketches are designed for particularly tall and skinny
matrices, one could argue that CountSketch failing above $d/n = 0.375$ is in
fact not problematic: on all the test instance which fall into the tall-and-
skinny setup, CountSketch retains rank with comparable distortion.

\begin{figure}
        \centering
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{subspace_embedding_results/subspace_embedding_dimension_gaussian_2048_256.pdf}
            \caption{$d=256$}
            \label{fig: subspace-125e3-aspectratio}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{subspace_embedding_results/subspace_embedding_dimension_gaussian_2048_512.pdf}
            \caption{$d=512$}
            \label{fig: subspace-375e1-aspectratio}
        \end{subfigure}
        \vskip\baselineskip
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{subspace_embedding_results/subspace_embedding_dimension_gaussian_2048_768.pdf}
            \caption{$d=768$}
            \label{fig: subspace-25e1-aspectratio}
        \end{subfigure}
        \quad
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{subspace_embedding_results/subspace_embedding_dimension_gaussian_2048_1024.pdf}
            \caption{$d=1024$}
            \label{fig: subspace-5e1-aspectratio}
        \end{subfigure}
        \caption{Distortion compared to sampling factor for various aspect ratios.
        Plotted are the mean results over 10 trials and a cross denotes when at least
        one trial has failed.
        The larger crosses indicate a higher number of failures with the largest
        crosses in Figure \ref{fig: subspace-5e1-aspectratio} for which CountSketch
        \textit{never} returned a full rank subspace embedding.}
        \label{fig: distortion-sampling-factor}
    \end{figure}



\subsection{Extra}
The CountSketch is noticeably faster than the SRHT (and we assume
 faster
than the Gaussian transform due to the explicit matrix product), the benefits
enjoyed by the time saving will become immaterial if the error induced by the
CountSketch is significantly higher.
In this experiment we test... and show that the subspace embedding error from
using the CountSketch is comparable to using both other methods.
There appears to be slightly more variation in the errors from using CountSketch
as opposed to the other methods and as such we need to check that this is not
prohibitive in the IHS model and this is explored in Section
\ref{sec: countsketch-ihs}.
