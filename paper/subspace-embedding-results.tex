%!TEX root = sketching-convex-ols.tex
\newcommand{\eat}[1]{}
\newcommand{\SRHT}{SRHT\xspace}

Our work is founded on the use of random projections to obtain
subspace embedding (Section~\ref{sec: preliminaries}).
In this section, we perform empirical studies to answer three central
questions:
(1) time cost: does the sketching approach provide speed-ups in
practice;
(2) accuracy: how large does the projection dimension need to be in
order to achieve small error;
(3) subspace embedding:
according to the theoretical analysis, the CountSketch requires a
higher projected dimension than other sketch methods to obtain a
subspace embedding -- but is this borne out in practice?
To address these three questions, we need to understand the constants inside
the $O(\cdot)$ terms required for a subspace embedding as described in Table
\ref{table: sketch-facts}.
Although a subspace embedding is sufficient to ensure convergence to the optimum,
in Section \ref{sec: ihs-rank-deficient} we will explore whether the solution can be learnt using
sketches which are as small as possible and may hence be rank deficient:
thus we need to explore when rank is preserved or lost.
We introduce the following definition which allows us to formally compare sketches
of a given projection dimension:

\begin{mydef} \label{def: sampling-factor}
The \textit{sampling factor} is the value $\gamma$ such that $m = \gamma d$
is the projection dimension for a random projection $SA \in R^{m \times d}$
of a matrix $A$.
\end{mydef}

\eat{
As outlined in Section \ref{sec: preliminaries} the key quantity that we would
like to understand is the subspace embedding.
The central questions we seek to answer are: (1) whether the time complexity
in practise matches the theory, (2) how large should the projection
dimension be in order to obtain small error (3) the CountSketch
theoretically requires a larger projection to obtain the subspace embedding
property, is this reflected in practise?}

\medskip
\noindent\textbf{Time cost for sparse data.}
We begin by evaluating the scalability of sketching for inputs that
are large and sparse.
We immediately remove the (dense) Gaussian sketch from consideration,
since the time cost for explicit matrix multiplication is excessively
high even for moderately sized inputs.
We subsequently focus our attention on CountSketch and \SRHT.
To construct a subspace embedding,
the CountSketch takes
time $O(\nnz{A})$, while the \SRHT requires $O(nd \log n)$.
That is, CountSketch explicitly scales (linearly) with the sparsity of
the data, whereas \SRHT, while fast, does not directly depend on the
sparsity.
We expect CountSketch to be preferable for very sparse data,
and investigate whether there is a point where the input density is
such that \SRHT is faster.

\noindent
\textit{Experimental setup.}
We generated random $n \times d$ matrices with $n=50,000$ and varying $d$
in $\{ 10, 100, 1000, 5000 \}$.
That is, the aspect ratio varies from $2 \times 10^{-4}$ to $1\times
10^{-1}$.
This is consistent with the range observed for the real data sets studied (see
Table \ref{table: data-facts}).
We varied the \textit{density} of the synthetic data.
We define density $\rho$ simply as the fraction of entries in the matrix $A$
that are non-zero.
We used the \texttt{scipy.sparse.random} routine to instantiate $A$,
where non-zero entries are chosen iid normal.
Note that the speed of our sketching methods does not depend on the data
distribution, so we are not overly concerned with this choice for
these timing experiments.
We initially choose the projection dimension $m = 5d$; this parameter
is investigated in more detail in subsequent sections.

\noindent
\textit{Timing results.}
Figure~\ref{fig: summary-time-50000} shows the timing results as
density varies for the four different input dimensions $d$.
The results appear consistent with the theory.
There is an increasing
trend for the time to build the CountSketch as the density of the data
increases.
Meanwhile, the time for the \SRHT embedding remains stable across
density values.
%
However, our main take-away from this experiment is that CountSketch
is \textit{an order of magnitude faster}, even in the densest regime.
In particular, we do not observe any cross over point where the \SRHT
embedding is faster to build than CountSketch for a given input
dimensionality.
We saw the same property for varying values of $n$, so we do not
report them.
Similar behavior is also seen on real datasets (Table
\ref{table: real-data-subspace-embedding}), where CountSketch is
uniformly faster than \SRHT, sometimes by multiple orders of
magnitude.
This is most pronounced for the ``tall-and-skinny'' (i.e. low aspect
ratio) data sets.
In absolute terms, the time cost to build the CountSketch is below one
second in all but the largest data sets.
Note that in a few experiments with large values of $d$ ($d>1000$),
the \SRHT failed to complete, and so results for these tests are
omitted.
The limiting factor appears to be the computation of Hadamard
transforms: these are individually relatively fast, but in aggregate create a
pinch point.


\begin{figure}
  \centering
\includegraphics[scale=0.75,keepaspectratio]{summary_time_density_50000}
        \caption{$n=50000$ and $d$ from $10,100,1000,5000$.  The legend
        describes the sketch used and the number appended is the value of $d$
        chosen for that particular experiment.
        A projection dimension of $m=5d$ was used.
        The mean time over 5 trials has been reported.}
        \label{fig: summary-time-50000}
\end{figure}





% \begin{figure}
%   \includegraphics[scale=0.25,keepaspectratio]{summary_time_density_10000}
%   \caption{Time to compute summaries for $n=100000$, $d$ given in legend next
%   to sketch name and sketch dimension $m = 5d$ used.} \label{fig: summary-times-vs-sparsity}
% \end{figure}




% \begin{figure}
%     \centering
%     \begin{subfigure}[b]{0.49\textwidth}
%         \includegraphics[scale=0.45,keepaspectratio]{summary_time_density_10000}
%         \caption{$n=100000$}
%         \label{fig: summary-time-100000}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.49\textwidth}
%         \includegraphics[scale=0.425,keepaspectratio]{distortion_vs_cols}
%         \caption{Distortion compared to number of columns}
%         \label{fig: distortion-columns}
%     \end{subfigure}
%     ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
%       %(or a blank line to force the subfigure onto a new line)
% \end{figure}

% Please add the following required packages to your document preamble:
%


\begin{table}[ht]
\centering
\begin{adjustbox}{width=1\textwidth}
\small

\begin{tabular}{|c|c|c|c|c|c|c||c|c|c|c|c|c|}
  \hline
\multirow{2}{*}{Dataset} & \multicolumn{3}{c|}{CountSketch (time)} & \multicolumn{3}{c||}{SRHT (time)} & \multicolumn{3}{c|}{CountSketch (error)} & \multicolumn{3}{c|}{SRHT (error)} \\
                        &     $1d$       &  $2d$            & $5d$          & $1d$          & $2d$        & $5d$         &     $1d$       &  $2d$          &   $5d$         &  $1d$          & $2d$         & $5d$  \\
\hline

Complex & 0.028 & 0.046 & 0.111 & 0.264 & 0.235 & 0.309 & $0.924_5$ & $0.464_5$ & $0.186_5$ & $0.829_5$ & $ 0.367_5$ & $0.0877_5$ \\

Landmark & 0.522 & 1.22 & 3.68 & - & - & - & $0.382_5$ & $0.190_5$ & $0.076_5$ & - & - & - \\

Slice &  0.067  &  0.073  &  0.117 &   1.34   & 1.29 & 1.27 &  $0.004_5$  &  $0.003_5$ &  $0.0006_5$ &   $0.004_5$ & $0.005_5$  &  $0.0005_5$  \\

Rail2586  & 0.897 & 1.31 & 3.15 & - & - & - & \textbf{0.085} & 0.043 & 0.017 & - & - & - \\

California Housing     &   0.001          &  0.001           &   0.002          & 0.018         &  0.022        & 0.017          &   \textbf{0.135}          &  0.024           & 0.008       & \textbf{0.242}          & 0.172         & 0.049 \\


YearPredictionsMSD      &    0.143         &   0.143          &  0.144          &  3.06         &  3.02        &   2.99        &  \textbf{0.031}           &   0.049          &    0.007           &   \textbf{0.033}        &   0.027        &  0.003         \\

US Census   & 0.261 & 0.257 & 0.240 & 7.08 & 7.23 & 6.62 & \textbf{0.032} &  0.152 & 0.037 & \textbf{0.063} & 0.081 & 0.041 \\

Susy  & 0.444 & 0.440 & 0.423 & 10.7 & 10.9 & 10.9 & \textbf{0.200} & 0.103 & 0.028 & \textbf{0.196} & 0.056 & 0.041 \\

\hline

w1a & 0.002 & 0.003 & 0.005 & 0.031 & 0.027 & 0.031 & $0.050_5$ & $0.026$ & $0.010_1$ & 0.047 & $0.026_2$ & $0.009_3$ \\

w2a & 0.001 & 0.002 & 0.004 & 0.032 & 0.033 & 0.037 & \textbf{0.052} & 0.030 & 0.009 & \textbf{0.047} & 0.024 & 0.007 \\

w3a & 0.001 & 0.002 & 0.006 & 0.066 & 0.058 & 0.073 & $0.048_2$ & $0.026_2$ & \textbf{0.001} & 0.057 & $0.024_2$ & \textbf{0.01} \\

w4a & 0.001 & 0.003 & 0.005 & 0.064 & 0.061 & 0.072 & 0.048 & $0.030_1$ & \textbf{0.011} & \textbf{0.049} & 0.024 & 0.001 \\

w5a & 0.002 & 0.003 & 0.005 & 0.126 & 0.133 & 0.138 & \textbf{0.047} & 0.024 & 0.009 & \textbf{0.051} & 0.024 & 0.010 \\

w6a & 0.002 & 0.004 & 0.006 & 0.329 & 0.298 & 0.331 & \textbf{0.053} & 0.022 & 0.010 & \textbf{0.050} &  0.026 & 0.012 \\

w7a & 0.003 & 0.004 & 0.006 & 0.369 & 0.366 & 0.342 & \textbf{0.050} & 0.024 & 0.011 & \textbf{0.049} & 0.023 & 0.010 \\

w8a & 0.005 & 0.006 & 0.010 & 0.956 & 0.930 & 0.994 & \textbf{0.052} & 0.024 & 0.010 & \textbf{0.046} & 0.024 & 0.010 \\
\hline
\end{tabular}
\end{adjustbox}
\caption{Comparison of subspace embedding results using CountSketch and SRHT
on real datasets.
Bold text indicates
the lowest projection dimension $m$ for which rank is preserved at this level
\textit{as well as all projection dimensions larger than $m$} in every trial.
We indicate the number of failures by a subscript: \textit{no} bold text and
\textit{no} subscript indicates that for this $m$ a subspace embedding was
achieved, however, for at least one of the larger projections there was a trial
in which rank was lost (for instance $(\text{w1a, SRHT}, 1d)$).
Observe that the three fail cases are the datasets  with the largest three
aspect ratios.}
\label{table: real-data-subspace-embedding}
\end{table}

% -----------------------------------------------------------------------------

\medskip
\noindent\textbf{Error and Embedding Dimension.}
\eat{
Although the theory shows that obtaining a subspace embedding is
a sufficient property to instantiate the IHS scheme,
the analysis does not reveal how big the \textit{sampling factor}
$\gamma$ must be relative to $d$ in order to ensure a subspace
embedding.
That is, we seek to empirically determine constants that are otherwise
hidden within the $O(\cdot)$ notation.
}
We study when the sketch $SA$ achieves full rank by varying the sampling factor
and measuring both distortion and rank.
Additionally, we vary input
parameters such as the aspect ratio of the matrix $A$ to observe the affect this
has on obtaining a subspace embedding.


\noindent
\textit{Experimental Setup.}
We carry out this experiment on synthetic data where we can easily
vary aspect ratio, and then confirm our findings on the real
datasets.
For this set of experiments we fix $n$ and then generate a selection of matrices
of full density
ranging from `tall-and-skinny' to `fat' matrices by choosing $d$ to vary
from roughly $0.05n$ to $0.5n$ over different distributions.
The empirical value of $\eps$ can be computed: this is referred to as the
\textit{distortion} and is measured by the error induced by the approximate matrix product
computation, as $\|A^T A - A^T S^T S A \|_F/\|A^TA\|_F$.
Results on the distortion for input matrices $A$ whose entries chosen iid normal are given
in Figure \ref{fig: distortion-sampling-factor}.
Each experiment is repeated 10 times, and the mean distortion reported.
We also measured the rank of the returned matrix $SA$, and discuss
when it is not preserved (i.e. $S$ does not provide a subspace
embedding).


\begin{figure}[t]
        \centering
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{subspace_embedding_results/subspace_embedding_dimension_gaussian_2048_256.pdf}
            \caption{$d=256$}
            \label{fig: subspace-125e3-aspectratio}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{subspace_embedding_results/subspace_embedding_dimension_gaussian_2048_512.pdf}
            \caption{$d=512$}
            \label{fig: subspace-375e1-aspectratio}
        \end{subfigure}
        \vskip\baselineskip
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{subspace_embedding_results/subspace_embedding_dimension_gaussian_2048_768.pdf}
            \caption{$d=768$}
            \label{fig: subspace-25e1-aspectratio}
        \end{subfigure}
        \quad
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{subspace_embedding_results/subspace_embedding_dimension_gaussian_2048_1024.pdf}
            \caption{$d=1024$}
            \label{fig: subspace-5e1-aspectratio}
        \end{subfigure}
        \caption{Distortion compared to sampling factor for various aspect ratios.
        Plotted are the mean results over 10 trials and a cross denotes when at least
        one trial has failed.
        The larger crosses indicate a higher number of failures with the largest
        crosses in Figure \ref{fig: subspace-5e1-aspectratio} for which CountSketch
        \textit{never} returned a full rank subspace embedding.}
        \label{fig: distortion-sampling-factor}
    \end{figure}

    \begin{figure}[t]
            \centering
            \begin{subfigure}[b]{0.475\textwidth}
                \centering
                \includegraphics[width=\textwidth]{subspace_embedding_results/subspace_embedding_dimension_exponential_2048_256.pdf}
                \caption{$d=256$}
                \label{fig: exp-subspace-125e3-aspectratio}
            \end{subfigure}
            \hfill
            \begin{subfigure}[b]{0.475\textwidth}
                \centering
                \includegraphics[width=\textwidth]{subspace_embedding_results/subspace_embedding_dimension_exponential_2048_512.pdf}
                \caption{$d=512$}
                \label{fig: exp-subspace-375e1-aspectratio}
            \end{subfigure}
            \vskip\baselineskip
            \begin{subfigure}[b]{0.475\textwidth}
                \centering
                \includegraphics[width=\textwidth]{subspace_embedding_results/subspace_embedding_dimension_exponential_2048_768.pdf}
                \caption{$d=768$}
                \label{fig: exp-subspace-25e1-aspectratio}
            \end{subfigure}
            \quad
            \begin{subfigure}[b]{0.475\textwidth}
                \centering
                \includegraphics[width=\textwidth]{subspace_embedding_results/subspace_embedding_dimension_exponential_2048_1024.pdf}
                \caption{$d=1024$}
                \label{fig: exp-subspace-5e1-aspectratio}
            \end{subfigure}
            \caption{
            Same setup as in Figure \ref{fig: distortion-sampling-factor} but
            data chosen iid from a standard Exponential distribution.
            Although the behaviour is more erratic than for the Gaussian distribution
            the general picture is similar, with the CountSketch performing
            similarly to the Gaussian sketch.
            The \SRHT appears to perform a constant factor better than both other
            methods.
            We also see similar rank loss as before,
            albeit with slightly better performance when $d/n = 0.25, 0.5$ compared
            to Figure \ref{fig: distortion-sampling-factor}.}
            \label{fig: exp-distortion-sampling-factor}
        \end{figure}


\noindent
\textit{Accuracy and Dimensionality Results: synthetic data.}
The collection of results are shown in Figures \ref{fig: distortion-sampling-factor}.
These plots indicate that, in general, the distortion induced by the CountSketch is
comparable with the Gaussian sketches.
However, both are a constant factor worse
than the \SRHT across each of the settings for this distribution.
In addition, the \SRHT and Gaussian sketches preserve the rank in
every instance.
However, this property is not observed when using the CountSketch.
In the small aspect ratio cases (i.e. Figures \ref{fig: subspace-125e3-aspectratio}
and \ref{fig: subspace-375e1-aspectratio}) a subspace embedding in all cases
except for the smallest sampling factor.
In contrast, when the aspect ratio exceeds 0.25, we see more rank deficiency for
lower sampling factors.
Here, we require $\gamma > 1.10$ to ensure CountSketch gives a
subspace embedding, but only $\gamma = 1.01$ for SRHT/Gaussian
sketches (Figure \ref{fig: subspace-375e1-aspectratio}).
This behaviour is exacerbated in the `fattest' case of $d/n = 0.5$
(Figure \ref{fig: subspace-5e1-aspectratio}),
when the CountSketch \textit{never} gives a subspace
embedding, while the other methods do.

Despite less reliably returing a subspace embedding,
the CountSketch achieves comparable distortion to the other methods and constructs
the sketch in vastly less time.
There is nevertheless a price to pay for this
computational speedup.
As the aspect ratio increases, the number of rank deficient embeddings also
grows; taking $\gamma > 1.05$ is sufficient to ensure that rank is preserved
when $d/n=0.25$ yet the required sampling factor grows considerably with $d/n$.
When $d/n=0.375$ we need roughly $\gamma = 1.125$ for all of the embeddings to
preserve rank for the CountSketch but when $d/n = 0.5$ \textit{none} of the
tested sampling factors up to $\gamma=1.25$ returned a subspace embedding.
%This failure of the CountSketch to accurately preserve rank at higher aspect
%ratios highlights one deficiency it possesses compared to the other sketches.
%Put simply
That is, \textit{CountSketch is more sensitive to higher aspect ratios and
requires a higher sampling factor in this regime compared to the other
sketching methods.}
However, given that the sketches are designed for particularly tall and skinny
matrices, one could argue that CountSketch's weaker reliability above
$d/n = 0.375$
is  not problematic: on all the test instances which fall into the tall-and-
skinny setup, CountSketch retains rank with comparable distortion.

\noindent
\textit{Accuracy and Dimensionality Results: real data.}
On real datasets we see similar behaviour.
CountSketch performs well when the aspect ratio is particularly small
but less reliably preserves rank as the aspect ratio increases.
For all but the highest aspect ratio datasets
there is a $\gamma$ such that sampling with at least this value
provides a subspace embedding.
For the highest aspect ratio datasets (Slice ($d/n = 7 \times 10^{-3}$),
 Complex ($d/n=1.2 \times 10^{-1}$),and w1a ($d/n = 1.2 \times
 10^{-1}$)), no method consistently finds a subspace embedding.
For tall-and-skinny datasets, CountSketch typically returns an
embedding at the same sampling factor as \SRHT,
and with distortion that differs by at most a small amount:  $10^{-2}$
in the worst case (California Housing, w3a), but more often on the order of $10^{-3}$.
This is apparent as we step through the w$n$a datasets;
neither method returns a full rank embedding for w$1$a beyond
 a certain sampling factor.
While both methods return a full rank embedding for w$3$a at $\gamma=5$,
 the w$4$a data requires $\gamma = 5$ for the CountSketch and $\gamma = 1$
 for \SRHT: this is the greatest disparity between the two methods and occurs
 at $d/n = 4.1 \times 10^{-2}$.


The theoretical analysis indicates that CountSketch should have a
linear dependence on the reciprocal failure probabilty ($1/\delta$),
compared to logarithmic for other methods.
Nevertheless, we do not see this clearly reflected in the empirical
behavior.
The largest aspect ratio for which the CountSketch behaves almost identically
 to \SRHT is $d/n = 3 \times 10^{-2}$: less than this and we see only small
 differences in the distortion of the embedding.
As a result, given the huge time saving, in the regime when
$d/n < 3 \times 10^{-2}$, the CountSketch seems preferable
since rank is retained at relatively low sampling factors.
If we are \textit{solely} interested in finding a subspace embedding for
slightly `fatter' data, along with less time constraints, then \SRHT is
 favourable.

\eat{
Despite this, in the iterative framework a rank-deficient `embedding' can
suffice, since directions that are missed in one iteration can be
found in later iterations.
Hence, CountSketch appears to be a good choice, due to its speed and
low distortion even when rank is lost.}

\medskip
\noindent
\textbf{Summary.}
We conclude  by returning to our original motivating questions.
We first observed that the CountSketch embedding scaled with the density of the
 data but this growth was mild enough that for all aspect ratios tested it was
 a factor of 10 faster than the SRHT (which also scaled consistently with the
 theory).
 Second, for a variety of real datasets we saw settings of the
 sampling factor $\gamma$ that ensure a subspace embedding is
 returned, and have shown examples when a `small' sampling factor is not sufficient.
Third, on the data we have tested, we have shown that there are examples
 when the CountSketch requires a larger embedding dimension to obtain a subspace
 embedding compared to \SRHT.
 However, this is only a small constant factor
 larger for CountSketch despite the theoretical analysis requiring a factor $d$ more.
 In addition, we saw empirically that there is a threshold below
 which the performance of the CountSketch is almost identical (in that rank is
 preserved and distortion is only a small constant factor different) to the \SRHT
 but is vastly cheaper to compute.
 Finally, for the aspect ratios tested we have also found thresholds at which
 rank is preserved or lost which will serve as a guide for Section
 \ref{sec: ihs-rank-deficient}.
