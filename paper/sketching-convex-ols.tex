\documentclass[twoside]{article}
%\usepackage{aistats2018}
 % If your paper is accepted, change the options for the package
% aistats2018 as follows:
%
%\usepackage[accepted]{aistats2018}

%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

%% Useful packages
\usepackage{a4wide}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{color}
\usepackage{url}
\usepackage{censor}
\usepackage{xspace}
\usepackage[normalem]{ulem}
\graphicspath{{../figures/}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbf{Var}}
\newcommand{\eps}{\varepsilon}
\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\nnz}[1]{\text{nnz}(#1)}

\theoremstyle{definition}\newtheorem{thm}{Theorem}[section]
\theoremstyle{definition}\newtheorem{mydef}[thm]{Definition}
\theoremstyle{definition}\newtheorem{rem}[thm]{Remark}
\theoremstyle{definition}\newtheorem{prop}[thm]{Proposition}
\theoremstyle{definition}\newtheorem{example}[thm]{Example}
\theoremstyle{definition}\newtheorem{claim}[thm]{Claim}
\theoremstyle{definition}\newtheorem{Qu}[thm]{Question}
\theoremstyle{definition}\newtheorem{Lemma}[thm]{Lemma}
\theoremstyle{definition}\newtheorem{Cor}[thm]{Corollary}
\theoremstyle{definition}\newtheorem{Fact}[]{Fact}

%% Useful commands
\DeclareMathOperator*{\argmin}{argmin}

%\author{Author, A.}
\title{Fast Preconditioning for Constrained Least Squares}
\author{Charlie Dickens}
\date{}


\begin{document}
\maketitle

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

%\twocolumn[

%\aistatstitle{Sparse Embeddings for Least Squares Regression with Convex Constraints}

%\aistatsauthor{ Author 1 \And Author 2 \And  Author 3 }

%\aistatsaddress{ Institution 1 \And  Institution 2 \And Institution 3 }
%]

\begin{abstract}
  Matrix sketching is a dimensionality reduction technique to make large datasets
  more manageable and has recently been appleid to solving a variety of
  convex constrained regression problems.
  There are two well-known sketching methods which can solve such
  regression problems; one being a sketch-and-
  solve type approach, and the other being an iterative scheme known as \textit{
  Iterative Hessian Sketching}.
  First we show that the
  Count Sketch \cite{clarkson2013low}, fits ideally within the iterative method
  and then give a suite of experiments to compare the performance of
  the CountSketch compared to competing methods.
  Our empirical results demonstrate that the Count Sketch is an extremely efficient
  sketch which offers \textit{computational} improvements over other sketching
  methods in this setup.
  \textit{Add more subject to experimental conclusion.}
\end{abstract}

NB. MAKE THE EMPHASIS THAT WE WANT TO SEE
HOW AGGRESSIVELY ONE CAN DOWNSAMPLE AND
STILL MAINTAIN A GOOD PRECONDITIONER IN
THE IHS SKETCHING MODEL.
PROVIDE THE SKETCH AND SOLVE MODEL FOR
COMPARISON.


\section{Introduction} \label{sec: intro}
\input{introduction}


\section{Preliminaries} \label{sec: preliminaries}
\input{preliminary}

\section{CountSketch in IHS: Theory} \label{sec: countsketch-properties}
\subsection{Experimental Outline}

\begin{enumerate}
  \item{\textbf{Summary Time vs Data Density.}  Compare running time for the
  summary computation as a function of data density.
  Do this on synthetic and real datasets.}
  \item{\textbf{Distortion.}  Given a sketch of fixed size, the distortion be
  well-understood.  Again refer to real and synthetic datasets.}
  \item{\textbf{Preliminary IHS experiments.}Do we lose anything by using the
  CountSketch in the IHS scheme? Compare solution error, prediction error}
  \item{\textbf{Case Study - LASSO.} Solve a LASSO
  instance on large datasets, how does the approximation compare to the SKLEARN
  implementation and the sketch-and-solve method}
\end{enumerate}




\subsection{Implementation Details}

All of our experiments are performed in the Anaconda distribution of Python
with only a few extra libraries.
We exploit a fast library for computing the Hadamard
Transform\footnote{https://bitbucket.org/vegarant/fastwht} and the Numba
library\footnote{https://numba.pydata.org/} to accelerate the Python code
so that it runs at comparable speed.
We test our algorithms on real and synthetic data which is summarised in Table
\ref{table: data-facts}.
Our code is made available at
\censor{\url{https://github.com/c-dickens/matrix_sketching}}\footnote{censore for review process}.

\begin{table}[ht]
\centering
\begin{adjustbox}{width=1\textwidth}
%\small

\begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
Dataset                   & Dimensionality  $(n,d)$      &   Aspect ratio $d/n$ & Density $\nnz{A}/nd$ &  Coherence Ratio & Rank & Source \\
\hline

Complex       & $(7350, 881)$         &   $1.2 \times 10^{-1} $      &  0.006                 & 1.07                & 881    &   \cite{davis2011university}      \\

Landmark        & $(71952, 2703)$         &   $4 \times 10^{-2} $      &  0.005                 & 41.8                & 2703    &   \cite{davis2011university}      \\

  Slice                   & $(53500, 387)$         &   $7 \times 10^{-3} $      &  0.36                 &                          910                 & 387    &   \cite{Dua:2017}       \\

Rail2586                 & $(923269, 2586)$      &   $2.7 \times 10^{-3} $      &  0.003                &  14840                & 2586    &   \cite{davis2011university} \\

California Housing       & $(20000, 17)$         &   $8.5 \times 10^{-4} $      &  0.76                 & 4261.4                 & 17    &   \cite{geron2017hands}      \\


YearPredictionsMSD       & $(515345,91)$         &   $1.7 \times 10^{-4} $      & 1.0                  &  2608.7                 &  91   &  \cite{Dua:2017}             \\

US Census                & $(5048299, 12)$         & $2.4 \times 10^{-6}$       &
0.5                 &    65.7           &  12 &  \cite{census2000}\\



Susy            & $(4999999, 19)$         &   $4 \times 10^{-6} $      &  0.96                 & 1791.0                & 19    &   \cite{Dua:2017}       \\








\hline

w1a & $(2477, 301)$    & $1.2 \times 10^{-1}$ & 0.041 & 779 & 301 & \cite{platt199912} \\

w2a & $(3470, 301)$    & $8.6 \times 10^{-2}$ & 0.042 & 1108 & 301 & \cite{platt199912} \\

w3a & $(4912, 301)$    & $6.1 \times 10^{-2}$ & 0.042 & 1626 & 301 & \cite{platt199912} \\

w4a & $(7366, 301)$    & $4.1 \times 10^{-2}$ & 0.042 & 2488 & 301 & \cite{platt199912} \\

w5a & $(9888, 301)$    & $3.0 \times 10^{-2}$ & 0.042 & 3418 & 301 & \cite{platt199912} \\

w6a & $(17188, 301)$ & $1.7 \times 10^{-2}$ & 0.042 & 6035 & 301 & \cite{platt199912} \\

w7a & $(24692, 301)$   & $1.2 \times 10^{-2}$ & 0.042 & 8684 & 301 & \cite{platt199912} \\

w8a & $(49749, 301)$    & $6 \times 10^{-3}$   & 0.042 & 17640 & 301 & \cite{platt199912} \\
\hline
\end{tabular}
\end{adjustbox}
\caption{Summary of real datasets used throughout (sorted by aspect ratio).
Coherence Ratio is the ratio of the largest and smallest leverage scores which
gives a measure of how uniformly distributed the leverage scores are.
Each of the w$n$a datasets is included as the number of columns remains constant
while the number of rows increases, while also retaining fairly consistent
data density.
As a result the aspect ratio decreases as the dataset is increased but the
coherence ratio increases: this is to be expected as many more rows are being
added without changing the rank so there must be rows with low leverage score.}
\label{table: data-facts}
\end{table}




\section{Embedding Properties} \label{sec: subspace-embedding-results}
\input{subspace-embedding-results}

\section{Learning with Rank Deficient Embeddings} \label{sec: ihs-rank-deficient}
\input{learning-without-rank}


\section{Input Sparsity IHS} \label{sec: countsketch-ihs}
\input{countsketch-verify-ihs}







\section{Case Study: LASSO} \label{sec: ihs-lasso}
\input{ihs-lasso}

\section{Case Study: SVM}

\subsection{Obtaining Machine Precision}

\subsection{Learning Rate}

\subsection{IHS as a fast approximate solver}


\appendix

\section{Structural Properties of CountSketch} \label{sec: countsketch-proofs}
\input{count-sketch-properties}

\bibliography{references}
\bibliographystyle{plain}
\end{document}
