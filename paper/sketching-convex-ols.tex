\documentclass[twoside]{article}
%\usepackage{aistats2018}
 % If your paper is accepted, change the options for the package
% aistats2018 as follows:
%
%\usepackage[accepted]{aistats2018}

%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

%% Useful packages
\usepackage{a4wide}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{color}
\usepackage[normalem]{ulem}
\graphicspath{{../figures/}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbf{Var}}
\newcommand{\eps}{\varepsilon}
\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\nnz}[1]{\text{nnz}(#1)}

\theoremstyle{definition}\newtheorem{thm}{Theorem}[section]
\theoremstyle{definition}\newtheorem{mydef}[thm]{Definition}
\theoremstyle{definition}\newtheorem{rem}[thm]{Remark}
\theoremstyle{definition}\newtheorem{prop}[thm]{Proposition}
\theoremstyle{definition}\newtheorem{example}[thm]{Example}
\theoremstyle{definition}\newtheorem{claim}[thm]{Claim}
\theoremstyle{definition}\newtheorem{Qu}[thm]{Question}
\theoremstyle{definition}\newtheorem{Lemma}[thm]{Lemma}
\theoremstyle{definition}\newtheorem{Cor}[thm]{Corollary}
\theoremstyle{definition}\newtheorem{Fact}[]{Fact}

%% Useful commands
\DeclareMathOperator*{\argmin}{argmin}

%\author{Author, A.}
\title{Exploiting Sparsity in Convex Constrained Least Squares Problems}
\author{Charlie Dickens}
\date{}


\begin{document}
\maketitle

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

%\twocolumn[

%\aistatstitle{Sparse Embeddings for Least Squares Regression with Convex Constraints}

%\aistatsauthor{ Author 1 \And Author 2 \And  Author 3 }

%\aistatsaddress{ Institution 1 \And  Institution 2 \And Institution 3 }
%]

\begin{abstract}
  Matrix sketching is a dimensionality reduction technique to make large datasets
  more manageable and has recently been appleid to solving a variety of
  convex constrained regression problems.
  There are two well-known sketching methods which can solve such
  regression problems; one being a sketch-and-
  solve type approach, and the other being an iterative scheme known as \textit{
  Iterative Hessian Sketching}.
  First we show that the
  Count Sketch \cite{clarkson2013low}, fits ideally within the iterative method
  and then give a suite of experiments to compare the performance of
  the CountSketch compared to competing methods.
  Our empirical results demonstrate that the Count Sketch is an extremely efficient
  sketch which offers \textit{computational} improvements over other sketching
  methods in this setup.
  \textit{Add more subject to experimental conclusion.}
\end{abstract}

NB. MAKE THE EMPHASIS THAT WE WANT TO SEE
HOW AGGRESSIVELY ONE CAN DOWNSAMPLE AND
STILL MAINTAIN A GOOD PRECONDITIONER IN
THE IHS SKETCHING MODEL.
PROVIDE THE SKETCH AND SOLVE MODEL FOR
COMPARISON.

Testing from Atom second time.

\section{Introduction} \label{sec: intro}
\input{introduction}


\section{Preliminaries} \label{sec: preliminaries}

Let $A \in \R^{n \times d}$ be the input data matrix and let $m$ be the
desired projection dimension.
We define the families of random matrices $S \in \R^{m \times n}$ that we will
study:
\begin{itemize}
  \item \textit{Gaussian} sketch: $G_{ij} \sim N(0,1)$ and the matrix
  $S = G/\sqrt{m}$
  \item \textit{Subsampled Randomized Hadamard Transform (SRHT)}: $S = PHD$
  where $D$ is a diagonal matrix with $D_{ii} \in \{ \pm 1 \}$ each with
  probability $1/2$ independently.
  The matrix $H$ is the recursively defined Hadamard Transform and $P$ is
  a matrix
  which samples rows uniformly at random.
  \item{\textit{TO INCLUDE: Sparse Johnson Lindenstrauss Transform (SJLT)}:}
  \item \textit{CountSketch}: initialise $S = \mathbf{0}_{m,n}$ and for every
  column $i$ of $S$ choose a row $h(i)$ uniformly at random.
  Set $S_{h(i),j} = \pm 1$ with probability $1/2$.
\end{itemize}

\textbf{Computational Complexity:}
Each of the above described random projections defines a linear map from $\R^n
\to \R^m$ which naively would take $O(mnd)$.
Despite this, only the Gaussian sketch suffers from the matrix multiplication
time cost because the SRHT exploits the fast Hadamard transform which takes
$O(nd \log n)$ time as it is defined recursively.
Additionally, the CountSketch can be computed by streaming through the matrix
$A$: upon observing an entry $A_{ij}$, the value of a hash bucket defined
by the function $h$ is then updated with either $\pm A_{h(i),j}$ and hence
the time
to compute the CountSketch transform is roughly $O(\nnz{A})$.
In light of this the matrix $SA$ can be viewed both as a linear transform but
also a method of summarising the input data to a small space representation.

\subsection{Setup}
Throughout we consider examples of overconstrained regression in the form of
a data matrix $A \in \R^{n \times d}$ and target vector $b \in \R^{n}$ with
$n \gg d$.
Also given is a set of convex constraints $\mathcal{C}$ and the task is to
find the \textit{optimal estimator}:

\begin{equation}
  x_{OPT} = \argmin_{x \in \mathcal{C}} \frac{1}{2} \|Ax-b\|_2^2.
\end{equation}

We consider two popular models of sketching as well as different types
of random projections.
The two methods are know as \textit{sketch-and-solve} and \textit{iterative
Hessian sketching} (IHS) which we now outline.
For a random projection $S$, the sketch-and-solve approach outputs estimates
of the form:

\begin{equation} \label{eq: sketch-and-solve}
  \hat{x}_S = \argmin_{x \in \mathcal{C}} \frac{1}{2} \|S(Ax-b)\|_2^2.
\end{equation}
\noindent In contrast, the IHS method instead sketches in the following manner:

\begin{equation} \label{eq: ihs}
  x^{t+1} = \argmin_{x \in \mathcal{C}}  \frac{1}{2} \|S^{t+1} A
  (x - x^t) \|^2 - \langle A^T (y - Ax^t), x - x^t \rangle.
\end{equation}

The reason for the differing approaches is due to the following notions of
approximation that we introduce as \textit{cost} and \textit{solution}
approximation.
Let $f(x) = \frac{1}{2}\|Ax-b\|_2^2$ for input parameters $(A,b)$ and let
$x_{OPT}$ denote the optimal solution.
We also use the following notation: $\|x\|_A = \frac{1}{\sqrt{n}}\|Ax\|_2$.

\begin{mydef} \label{def: cost-approx}
  An algorithm which outputs $\hat{x}$ is a $(1 + \eps)$ \textit{cost
   approximation} if $f(\hat{x}) \le (1+\eps) f(x_{OPT})$.
\end{mydef}

\begin{mydef} \label{def: sol-approx}
  An algorithm which returns $\hat{x}$ such that $\|x_{OPT} - \hat{x}\|_A
  \le \eps \|x_{OPT}\|_2$ is referred to as a $\eps$-\textit{solution
  approximation} algorithm.
\end{mydef}

Inspecting Equations (\ref{eq: sketch-and-solve}) and (\ref{eq: ihs}) one
sees that that a key quantity we need to understand is the concept
of a \textit{subspace embedding} which measures the distortion induced by the
data summary on vectors.
Each of the sketches defined in Section \ref{sec: intro} achieves the subspace
embedding criterion which is summarised in Theorem
\ref{thm: subspace-embedding-dims}.

\begin{mydef} \label{def: subspace-embedding}
  A matrix $S \in \R^{m \times n}$ is a \textit{$(1 \pm \eps)$-subspace embedding}
  for the column space of a matrix $A \in \R^{n \times d}$ if for all vectors
  $x \in \R^d, \|SAx\|_2^2 = (1 \pm \eps)\|Ax\|_2^2$.
\end{mydef}

\begin{rem} \label{rem: subspace-embedding-remark}
  $\log(1/\delta)$ failure probability dependence compared to $O(1/\delta)$
  and the theoretically increased projection dimension for subspace embedding.
\end{rem}



\begin{thm}[\cite{woodruff2014sketching}] \label{thm: subspace-embedding-dims}
  Let $A \in \R^{n \times d}$ have full column rank.
  Let $S \in \R^{m \times n}$ be sampled from one of the Gaussian, SRHT, or
  CountSketch distributions.
  Then to achieve the subspace embedding property with probability
  $1 - \delta$ we require:
  $m = O(\eps^{-2}(d + \log(1/\delta)))$ for the Gaussian sketch,
  $m = \Omega(\eps^{-2}(\log d) (\sqrt{d} + \sqrt{n})^2)$ for the SRHT,
  $m = O(d^2/(\delta \eps^2))$ for the CountSketch.
\end{thm}



\textbf{Questions to answer}:
\begin{itemize}
  \item Why solution approimation?
  \item IHS with one iteration.
  \item Order one bound error comment from IHS Eqn 5
  \item Comments on the IHS algorithm, iterations etc.
  \item \sout{Hesian Sketch as preconditioner}
  \item Complexity arguments
  \item need $d + \log n$ in each round for IHS.
\end{itemize}

\color{red}
It has also been argued in \cite{wang2017sketching} that IHS is acting as
a preconditioned approach to the Newton method.
Indeed, for $f(x) = \frac{1}{2} \|Ax-b\|_2^2$ we see that $\nabla f(x) =
-A^T(b-Ax)$.
In the iterates, the task is to find $u' = \argmin_{x \in \mathcal{C}}
\frac{1}{2}\|SAu\|_2^2 - u^TA^T(b-Ax_t)$.
By solving the unconstrained problem one would obtain $u' = (A^TS^TSA)^{-1}
A^T(b-Ax_t)$ and thus, $u' = - \tilde{H}^{-1} \nabla f(x_t)$.
Then taking $x^{t+1} = \mathcal{P}_{\mathcal{C}} (x^t + u')$ we obtain
$x^{t+1} = \mathcal{P}_{\mathcal{C}} (x^t - \tilde{H}^{-1}\nabla f(x_t))$
which is a projected Newton step with a sketched approximation to
the Hessian.
\color{black}

\subsection{Motivating the IHS: Sketch-and-Solve Suboptimality}

For a variety of the results we asume the standard Gaussian design.
stating that $y = A x^* + \omega$.
Here, the data $A$ is fixed and there exists an unknown ground truth
vector $x^*$ belonging to some compact $\mathcal{C}_0 \subseteqq \mathcal{C}$.
In addition, the error vector $\omega$ has entries are drawn
by $\omega_i \sim N(0, \sigma^2)$.
In order to compare the output of an algorithm, $\hat{x}$, to that of the
`exact` (optimal / best ??!!) estimator, $x_{OPT}$ Pilanci and Wainwright
showed that the expected solution error could be bounded above as a function
of the input size and from below as
In addition, it was shown that any estimator $x'$ which observes the pair
$(SA, Sb)$ is provably suboptimal in the sense outlined in Theorem
\ref{thm: clsq-lower-bound}.
The key condition is that a sketching matrix has the following
property (in spectral norm):

\begin{equation} \label{eq: sketch-spectral-property}
  \| \E \left[ S^T (S S^T)^{-1} S \right] \|_2 \le \eta \frac{m}{n}
\end{equation}

\begin{thm}[\cite{pilanci2016iterative}] \label{thm: clsq-lower-bound}
  Let $S \in \R^{m \times n}$ be a sketching matrix which satisfies
  Equation (\ref{eq: sketch-spectral-property}), then any estimator
  based on observing $(SA, Sb)$ has solution error lower bounded as:

  \begin{equation}
    \sup_{x^* \in \mathcal{C}_0} \E_{S,\omega} [\|x^* - x'\|_A^2 ]
     = \Omega \left(
    \frac{\sigma^2}{\eta \min(m,n)} \right).
  \end{equation}
\end{thm}

\begin{rem}
  We note that the constants for Theorem \ref{thm: clsq-lower-bound} are
  explicitly known and are simply the factor $\log(0.5 M_{0.5})/128\eta$
  which we omit for brevity.
  Note that $M_{0.5}$ is the $0.5$-packing number of $\mathcal{C}_0$ in
  the semi-norm $\| \cdot \|_A$.
\end{rem}
Although we omit the details (which can be found in \cite{pilanci2016iterative})
we note that the CountSketch satisfies the spectral condition (\ref{eq: sketch-spectral-property})
and as such, suffers from the same suboptimality as the previously used
sketching methods in the sketch-and-solve model.
This is proved in Theorem \ref{thm: spectral-theorem} and hence motivates the
novel use of CountSketch within the IHS framework rather than the straightforward
sketch-and-solve model.

\subsection{Iterative Hessian Sketch}
\begin{enumerate}
  \item{Convergence theorems}
  \item{Sketch dimensions}
  \item{Number of iterations}
\end{enumerate}

\begin{prop}[\cite{pilanci2016iterative}] \label{prop: ihs-error-bound}
  For any set $\mathcal{C}$ containing $x^*$, the constrained least-squares
  estimate $x_{OPT}$ has prediction error upper bounded as:

  \begin{equation}
    \E \|x_{OPT} - x^* \|_A^2 \le c \left(\eps_n(\mathcal{\bar{K}}) +
    \frac{\sigma^2}{n}\right).
  \end{equation}
  For the unconstrained case we have the prediction error is $O(\sigma^2 d/n)$.
\end{prop}



\subsection{Experimental Outline}

\begin{enumerate}
  \item{\textbf{Summary Time vs Data Density.}  Compare running time for the
  summary computation as a function of data density.
  Do this on synthetic and real datasets.}
  \item{\textbf{Distortion.}  Given a sketch of fixed size, the distortion be
  well-understood.  Again refer to real and synthetic datasets.}
  \item{\textbf{Preliminary IHS experiments.}Do we lose anything by using the
  CountSketch in the IHS scheme? Compare solution error, prediction error}
  \item{\textbf{Case Study - LASSO.} Solve a LASSO
  instance on large datasets, how does the approximation compare to the SKLEARN
  implementation and the sketch-and-solve method}
\end{enumerate}

\section{CountSketch in IHS: Theory}
The proofs detailing the structural results of why we can use the CountSketch
within the IHS are given in Appendix \ref{sec: countsketch-proofs} however we
outline the key details here.
First we need that the sketches are zero mean and have identity covariance
$\E(S^TS) = I_{n}$ which is shown in Lemma \ref{lem: covariance_matrix}.
Two further properties are required for the error bounds of the IHS.

\begin{mydef}
  The \textit{tangent cone} is the following set:
  \begin{equation}
    K = \{ v \in \R^d : v = tA(x-x_{OPT}), ~ t \ge 0, x \in \mathcal{C}\}
  \end{equation}
\end{mydef}
\noindent We note that the residual error vector for an approximation $\hat{x}$
belongs to this set as $u=A(\hat{x} - x_{OPT})$.
Let $X = K \cap \mathcal{S}^{n-1}$ where $\mathcal{S}^{n-1}$ is the set of $n$-
dimensional vectors which have unit Euclidean norm.
The quanitites we need to understand are:

\begin{align}
  Z_1 &= \inf_{u \in X} \|Su\|_2^2 \\
  Z_2 &= \sup_{u \in X} | u^T S^T S v - u^T v |.
\end{align}
\noindent Here, $v$ denotes a fixed unit-norm $n$-dimensional vector and assume
that $S$ is a subspace embedding for the column space of $A$.
For the IHS conditions to hold it is required that $Z_1 \ge 1-\eps$ and
$Z_2 \le \eps/2$ for $\eps \in (0,1/2)$.
Since $ u \in K \subseteq \text{col}(A)$ and then by the subspace embedding
property we have that $\|Su\|_2^2 = (1 \pm \eps)\|u\|_2^2 = (1 \pm \eps)$ and
hence $Z_1  \ge 1-\eps$ as required.
The second property follows from approximate matrix product with sketching
matrices:
indeed, for a matrix $X$ with only one nonzero $X_{ij}$ then $\|X\|_F = (X_{ij}^2)^{1/2}$
which is exactly $|X_ij|$.
Pad $u$ and $v$ with zeros until they are both $d \times k$ dimensional, to get
$\mat{u}$ and $\mat{v}$ respectively so that we can now apply the matrix product
(Theorem 13 of \cite{woodruff2014sketching} but originally \cite{kane2014sparser}).
This gives $\|\mat{u}^T S^TS \mat{v} - \mat{u}^T\mat{v}\|_F \le 3 \eps \|\mat{u}\|_F \|\mat{v}\|_F$
for $\eps \in (0,1/2)$.
Now, by rescaling $\eps$, recalling that the definitions of $\mat{u},\mat{v},u,v$
mean $\|\mat{u}\|_F = 1, \|\mat{v}\|_F=1$ and then error difference has exactly
one element so therefore reduces to the absolute value of that element, which is
exactly $| u^T S^T S v - u^T v |$ and hence $Z_2 \le \eps$.
Note that although the condition on $Z_2$ looks like a Johnson Lindenstrauss
Transform property, we cannot immediately invoke that for the CountSketch in the
general case due to a lower bound on the column sparsity needed for  a JLT
(discussion of this can be found in \cite{woodruff2014sketching} between Theorems
7 and 8).
Coupled with the fact that the rows of a CountSketch matrix $S$ are sub-Gaussian,
we are now free to use the CountSketch within the IHS framework.





\subsection{Implementation Details}

All of our experiments are performed in the Anaconda distribution of Python
with only a few extra libraries.
We exploit a fast library for computing the Hadamard
Transform\footnote{https://bitbucket.org/vegarant/fastwht} and the Numba
library\footnote{https://numba.pydata.org/} to accelerate the Python code
so that it runs at comparable speed.



\section{Warm up: initial comparisons}
\input{subspace-embedding-results}


\section{CountSketch in the IHS framework} \label{sec: countsketch-ihs}
\input{countsketch-verify-ihs}




\subsection{Time}
Vary sparsity.
\subsection{Space}

\subsection{Distortion}



\section{Case Study: LASSO}
\input{ihs-lasso}

\subsection{Obtaining Machine Precision}

\subsection{Learning Rate}

\subsection{IHS as a fast approximate solver}

\section{Case Study: SVM}


\appendix

\section{Structural Properties of CountSketch} \label{sec: countsketch-proofs}
\input{count-sketch-properties}




\bibliography{references}
\bibliographystyle{plain}
\end{document}
